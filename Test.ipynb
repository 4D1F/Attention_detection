{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import model_from_json\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "\n",
    "emotion_dict = {0: \"Happy\", 1: \"Neutral\", 2: \"Sad\"}\n",
    "\n",
    "# load json and create model\n",
    "json_file = open('emotion_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "emotion_model = model_from_json(loaded_model_json)\n",
    "\n",
    "# load weights into new model\n",
    "emotion_model.load_weights(\"emotion_model.h5\")\n",
    "print(\"Loaded model from disk\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_face(img):\n",
    "    face_cascade = cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_default.xml')   \n",
    "    face_img = img.copy()\n",
    "    face_rects = face_cascade.detectMultiScale(face_img,scaleFactor=1.3, minNeighbors=5)\n",
    "    for (x,y,w,h) in face_rects:\n",
    "        cv2.rectangle(face_img,(x,y),(x+w,y+h),(0,255,0),5)\n",
    "\n",
    "    return (x,y,w,h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'x' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [4], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m ret, frame \u001b[39m=\u001b[39m cap\u001b[39m.\u001b[39mread()\n\u001b[0;32m     31\u001b[0m frame \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mresize(frame, (\u001b[39m1080\u001b[39m, \u001b[39m720\u001b[39m))\n\u001b[1;32m---> 32\u001b[0m roi \u001b[39m=\u001b[39m detect_face(frame)\n\u001b[0;32m     33\u001b[0m ret \u001b[39m=\u001b[39m tracker\u001b[39m.\u001b[39minit(frame, roi)\n",
      "Cell \u001b[1;32mIn [3], line 8\u001b[0m, in \u001b[0;36mdetect_face\u001b[1;34m(img)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m (x,y,w,h) \u001b[39min\u001b[39;00m face_rects:\n\u001b[0;32m      6\u001b[0m     cv2\u001b[39m.\u001b[39mrectangle(face_img,(x,y),(x\u001b[39m+\u001b[39mw,y\u001b[39m+\u001b[39mh),(\u001b[39m0\u001b[39m,\u001b[39m255\u001b[39m,\u001b[39m0\u001b[39m),\u001b[39m5\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m \u001b[39mreturn\u001b[39;00m (x,y,w,h)\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'x' referenced before assignment"
     ]
    }
   ],
   "source": [
    "# tracker = cv2.legacy.TrackerKCF_create()\n",
    "tracker = cv2.TrackerCSRT_create()\n",
    "tracker_name = str(tracker).split()[0][8:]\n",
    "\n",
    "# happy = 0.9\n",
    "# sad = 0.8\n",
    "# neutral = 1.0\n",
    "\n",
    "happy_count = 0\n",
    "sad_count = 0\n",
    "neutral_count = 0\n",
    "face_front = 0\n",
    "frame_count = 0\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "# cap = cv2.VideoCapture(\"video2.mp4\")\n",
    "\n",
    "# while True:\n",
    "#     ret, frame = cap.read()\n",
    "#     if not ret:\n",
    "#         continue\n",
    "#     frame = cv2.resize(frame, (1080, 720))\n",
    "#     try:\n",
    "#         roi = detect_face(frame)\n",
    "#         break\n",
    "#     except Exception as e:\n",
    "#         print(\"Couldn't detect face, trying again!\")\n",
    "#         print(e)\n",
    "        # cap.release()\n",
    "ret, frame = cap.read()\n",
    "frame = cv2.resize(frame, (1080, 720))\n",
    "roi = detect_face(frame)\n",
    "ret = tracker.init(frame, roi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenCV(4.5.5) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\resize.cpp:4052: error: (-215:Assertion failed) !ssize.empty() in function 'cv::resize'\n",
      "\n",
      "38\n",
      "37\n",
      "Happy =  4\n",
      "Sad =  7\n",
      "Neutral =  26\n",
      "Average Emotion Index =  0.9263157894736843\n",
      "Front View Rate =  0.9736842105263158\n",
      "Attention Rate =  90.19390581717452\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    while True:\n",
    "        frame_count += 1\n",
    "\n",
    "        ret, frame = cap.read()\n",
    "        frame = cv2.resize(frame, (1080, 720))\n",
    "        # frame = cv2.flip(frame, 1)\n",
    "\n",
    "        success, roi = tracker.update(frame)\n",
    "\n",
    "        (x,y,w,h) = tuple(map(int,roi))\n",
    "        \n",
    "        if not ret:\n",
    "            break\n",
    "        # face_detector = cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_default.xml')\n",
    "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # # detect faces available on camera\n",
    "        # num_faces = face_detector.detectMultiScale(gray_frame, scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "        # # take each face available on the camera and Preprocess it\n",
    "        # for (x, y, w, h) in num_faces:\n",
    "        cv2.rectangle(frame, (x, y-50), (x+w, y+h), (0, 255, 0), 4)\n",
    "        roi_gray_frame = gray_frame[y:y + h, x:x + w]\n",
    "        cropped_img = np.expand_dims(cv2.resize(roi_gray_frame, (48, 48)), 0)\n",
    "\n",
    "        if len(cropped_img) > 0:\n",
    "            face_front += 1\n",
    "\n",
    "        # predict the emotions\n",
    "        emotion_prediction = emotion_model.predict(cropped_img, verbose=0)\n",
    "        maxindex = int(np.argmax(emotion_prediction))\n",
    "        cv2.putText(frame, emotion_dict[maxindex], (x+5, y-20), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "        if emotion_dict[maxindex] == 'Happy':\n",
    "            happy_count += 1\n",
    "        elif emotion_dict[maxindex] == 'Sad':\n",
    "            sad_count += 1\n",
    "        elif emotion_dict[maxindex] == 'Neutral':\n",
    "            neutral_count += 1\n",
    "\n",
    "            \n",
    "        # Draw Rectangle as Tracker moves\n",
    "        if success:\n",
    "            # Tracking success\n",
    "            p1 = (x, y)\n",
    "            p2 = (x+w, y+h)\n",
    "            cv2.rectangle(frame, p1, p2, (0,0,255), 3)\n",
    "        else :\n",
    "            # Tracking failure\n",
    "            cv2.putText(frame, \"Failure to Detect Tracking!!\", (100,200), cv2.FONT_HERSHEY_SIMPLEX, 1,(0,0,255),3)\n",
    "\n",
    "        # Display tracker type on frame\n",
    "        cv2.putText(frame, tracker_name, (20,400), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0),3)    \n",
    "\n",
    "        cv2.imshow(tracker_name, frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(frame_count)\n",
    "print(face_front)\n",
    "print(\"Happy = \", happy_count)\n",
    "print(\"Sad = \", sad_count)\n",
    "print(\"Neutral = \", neutral_count)\n",
    "\n",
    "avg_emotion = (happy_count * 0.9 + sad_count * 0.8 + neutral_count) / frame_count\n",
    "print(\"Average Emotion Index = \", avg_emotion)\n",
    "front_view = face_front/frame_count\n",
    "print(\"Front View Rate = \", front_view)\n",
    "\n",
    "attention = (avg_emotion * front_view) * 100\n",
    "\n",
    "print(\"Attention Rate = \", attention)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a9cff5a362bc38ef45d817ae74b1af54d6a076e3d773891282bce078b815ba34"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
